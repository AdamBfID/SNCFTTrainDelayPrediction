# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CygXIn0pkBrd1yyzUk2PTYCsnVcl_4ol
"""

import pandas as pd
import numpy as np

schecule = pd.read_csv('schecule.csv')
schecule.head()

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

# Station sequence from the schedule
stations = [
    'ERRIADH', 'BORJ CEDRIA', 'BIR BEY', 'HAMMAM CHATT', 'TAHAR SFAR',
    'ARRET DU STADE', 'HAMMAM LIF', 'BOUKORNINE', 'LYCEE EZZAHRA',
    'EZ-ZAHRA', 'RADES MELIANE', 'RADES', 'LYCEE RADES', 'SIDI REZIG',
    'MEGRINE', 'MEGRINE RYADH', 'JEBEL JELLOUD', 'TUNIS VILLE'
]

# Train schedule with day categories
# A: All days except Sundays and holidays
# B: Sundays and holidays only
# C: All days except Saturdays, Sundays and holidays (weekdays only)
# D: All days (every day)
train_day_categories = {
    102: 'D', 104: 'A', 106: 'B', 108: 'A', 110: 'D', 114: 'A', 116: 'B', 118: 'A',
    120: 'A', 122: 'B', 124: 'A', 126: 'C', 128: 'B', 130: 'A', 132: 'A', 136: 'D',
    138: 'A', 140: 'B', 142: 'A', 146: 'D', 148: 'D', 150: 'A', 152: 'D', 154: 'D',
    156: 'D', 158: 'D', 160: 'D', 164: 'D', 168: 'D', 172: 'D', 176: 'D', 178: 'C',
    184: 'D', 186: 'C', 190: 'D', 194: 'D', 198: 'D', 206: 'D', 208: 'C', 210: 'C',
    212: 'D', 216: 'D', 220: 'D', 222: 'C', 228: 'D', 230: 'D', 234: 'D', 236: 'D',
    240: 'D', 242: 'D', 246: 'D', 250: 'D'
}

# Fixed schedule data with corrected time formats
schedule_data = {
    102: ['4:30', '4:35', '4:38', '4:40', '4:43', '4:45', '4:48', '4:51', '4:53', '4:55', '4:58', '5:01', '', '5:05', '5:08', '5:10', '5:12', '5:21'],
    104: ['4:50', '4:55', '4:58', '5:00', '5:03', '5:05', '5:08', '5:11', '5:13', '5:15', '5:18', '5:21', '', '5:25', '5:28', '5:30', '5:32', '5:41'],
    106: ['5:00', '5:05', '5:08', '5:10', '5:13', '5:15', '5:18', '5:21', '5:23', '5:25', '5:28', '5:31', '', '5:35', '5:38', '5:40', '5:42', '5:51'],
    108: ['5:10', '5:15', '5:18', '5:20', '5:23', '5:25', '5:28', '5:31', '5:33', '5:35', '5:38', '5:41', '', '5:45', '5:48', '5:50', '5:52', '6:01'],
    110: ['5:35', '5:40', '5:43', '5:45', '5:48', '5:50', '5:53', '5:56', '5:58', '6:00', '6:03', '6:06', '', '6:10', '6:13', '6:15', '6:17', '6:26'],
    114: ['5:55', '6:00', '6:03', '6:05', '6:08', '6:10', '6:13', '6:16', '6:18', '6:20', '6:23', '6:26', '6:28', '6:31', '6:33', '6:35', '6:38', '6:46'],
    116: ['6:00', '6:05', '6:08', '6:09', '6:11', '6:14', '6:17', '6:19', '6:21', '6:24', '6:27', '6:30', '6:32', '6:34', '6:37', '6:39', '6:41', '6:50'],
    118: ['6:10', '6:15', '6:18', '6:19', '6:21', '6:23', '6:26', '6:28', '6:30', '6:32', '6:34', '6:37', '', '', '', '', '', '6:52'],
    120: ['6:25', '6:30', '6:33', '6:35', '6:38', '6:40', '6:43', '6:46', '6:48', '6:50', '6:53', '6:56', '6:58', '7:01', '7:03', '7:05', '7:08', '7:16'],
    122: ['6:30', '6:35', '6:38', '6:40', '6:43', '6:45', '6:48', '6:51', '6:53', '6:55', '6:58', '7:01', '7:03', '7:06', '7:08', '7:10', '7:13', '7:21'],
    124: ['6:40', '6:45', '6:48', '6:50', '6:52', '6:55', '6:58', '7:00', '7:02', '7:05', '7:08', '7:11', '7:13', '7:15', '7:17', '7:19', '7:22', '7:30'],
    126: ['6:55', '7:00', '7:03', '7:05', '7:08', '7:10', '7:13', '', '', '7:18', '', '7:23', '', '', '', '', '', '7:38'],
    128: ['7:00', '7:05', '7:09', '7:11', '7:13', '7:16', '7:19', '7:21', '7:23', '7:26', '7:29', '7:32', '7:35', '7:37', '7:40', '7:42', '7:45', '7:53'],
    130: ['7:05', '7:10', '7:13', '7:15', '7:17', '7:20', '7:23', '7:25', '7:27', '7:30', '7:33', '7:36', '7:38', '7:41', '7:43', '7:45', '7:48', '7:56'],
    132: ['', '', '', '', '', '', '7:35', '7:37', '7:39', '7:42', '7:45', '7:48', '7:50', '7:53', '7:55', '7:57', '7:59', '8:07'],
    136: ['7:30', '7:35', '7:39', '7:41', '7:43', '7:45', '7:48', '7:50', '7:52', '7:55', '7:58', '8:01', '8:03', '8:06', '8:08', '8:10', '8:13', '8:21'],
    138: ['7:50', '7:55', '7:58', '8:00', '8:03', '8:05', '8:08', '8:11', '8:13', '8:15', '8:18', '8:21', '8:23', '8:26', '8:28', '8:30', '8:33', '8:41'],
    140: ['8:00', '8:05', '8:08', '8:10', '8:13', '8:15', '8:18', '8:21', '8:23', '8:25', '8:28', '8:31', '8:33', '8:36', '8:38', '8:40', '8:43', '8:51'],
    142: ['8:10', '8:15', '8:18', '8:20', '8:23', '8:25', '8:28', '8:31', '8:33', '8:35', '8:38', '8:41', '8:43', '8:46', '8:48', '8:50', '8:53', '9:01'],
    146: ['8:30', '8:35', '8:38', '8:40', '8:42', '8:45', '8:48', '8:50', '8:52', '8:55', '8:58', '9:01', '9:03', '9:05', '9:08', '9:10', '9:12', '9:20'],
    148: ['9:00', '9:05', '9:08', '9:10', '9:13', '9:15', '9:18', '9:21', '9:23', '9:25', '9:28', '9:31', '9:34', '9:36', '9:39', '9:41', '9:43', '9:51'],
    150: ['9:15', '9:20', '9:23', '9:25', '9:28', '9:30', '9:33', '9:36', '9:38', '9:40', '9:43', '9:46', '9:48', '9:51', '9:53', '9:55', '9:58', '10:06'],
    152: ['9:30', '9:35', '9:38', '9:40', '9:43', '9:45', '9:48', '9:51', '9:53', '9:55', '9:58', '10:01', '10:03', '10:06', '10:08', '10:10', '10:13', '10:21'],
    154: ['10:00', '10:05', '10:08', '10:10', '10:13', '10:15', '10:18', '10:21', '10:23', '10:25', '10:28', '10:31', '10:33', '10:36', '10:38', '10:40', '10:43', '10:51'],
    156: ['10:30', '10:35', '10:38', '10:40', '10:43', '10:45', '10:48', '10:51', '10:53', '10:55', '10:58', '11:01', '11:03', '11:06', '11:08', '11:10', '11:13', '11:21'],
    158: ['11:00', '11:05', '11:08', '11:10', '11:13', '11:15', '11:18', '11:21', '11:23', '11:25', '11:28', '11:31', '11:33', '11:36', '11:38', '11:40', '11:42', '11:50'],
    160: ['11:30', '11:35', '11:38', '11:40', '11:43', '11:45', '11:48', '11:51', '11:53', '11:55', '11:58', '12:01', '12:03', '12:06', '12:08', '12:10', '', '12:20'],
    164: ['12:00', '12:05', '12:08', '12:10', '12:12', '12:14', '12:17', '12:19', '12:21', '12:23', '12:26', '12:29', '12:31', '12:33', '12:36', '12:37', '12:39', '12:48'],
    168: ['12:30', '12:35', '12:38', '12:40', '12:43', '12:45', '12:48', '12:51', '12:53', '12:55', '12:58', '13:01', '13:03', '13:06', '13:08', '13:10', '13:13', '13:21'],
    172: ['13:00', '13:05', '13:08', '13:10', '13:13', '13:15', '13:18', '13:21', '13:23', '13:25', '13:28', '13:31', '13:34', '13:36', '13:39', '13:41', '13:43', '13:51'],
    176: ['13:30', '13:35', '13:38', '13:40', '13:43', '13:45', '13:48', '13:51', '13:53', '13:55', '13:58', '14:01', '14:03', '14:06', '14:08', '14:10', '14:13', '14:21'],
    178: ['13:45', '13:50', '13:53', '13:55', '13:58', '14:00', '14:03', '14:06', '14:08', '14:10', '14:13', '14:16', '14:18', '14:21', '14:23', '14:25', '14:27', '14:36'],
    184: ['14:00', '14:05', '14:08', '14:10', '14:13', '14:15', '14:18', '14:21', '14:23', '14:25', '14:28', '14:31', '14:33', '14:36', '14:38', '14:40', '14:42', '14:50'],
    186: ['14:20', '14:25', '14:28', '14:30', '14:33', '14:35', '14:38', '14:41', '14:43', '14:45', '14:48', '14:51', '14:53', '14:56', '14:58', '15:00', '15:02', '15:10'],
    190: ['14:30', '14:35', '14:38', '14:40', '14:43', '14:45', '14:48', '14:51', '14:53', '14:55', '14:58', '15:01', '15:03', '15:06', '15:08', '15:10', '15:13', '15:21'],
    194: ['15:00', '15:05', '15:08', '15:10', '15:13', '15:15', '15:18', '15:21', '15:23', '15:25', '15:28', '15:31', '', '15:34', '15:36', '15:39', '15:41', '15:43'],
    198: ['15:30', '15:35', '15:38', '15:40', '15:43', '15:45', '15:48', '15:51', '15:53', '15:55', '15:58', '16:01', '16:03', '16:06', '16:08', '16:10', '16:13', '16:21'],
    206: ['16:00', '16:05', '16:08', '16:10', '16:13', '16:15', '16:18', '16:21', '16:23', '16:25', '16:28', '16:31', '', '16:33', '16:36', '16:38', '16:40', '16:43'],
    208: ['', '', '', '', '', '', '16:25', '16:27', '16:29', '16:31', '16:34', '16:38', '16:40', '16:42', '16:44', '16:45', '16:48', '16:56'],
    210: ['16:20', '16:25', '16:28', '16:29', '16:31', '16:33', '16:36', '', '', '16:41', '', '16:46', '', '', '', '', '', '17:01'],
    212: ['16:30', '16:35', '16:38', '16:40', '16:43', '16:45', '16:48', '16:51', '16:53', '16:55', '16:58', '17:01', '17:03', '17:06', '17:08', '17:10', '17:13', '17:21'],
    216: ['17:00', '17:05', '17:08', '17:10', '17:13', '17:15', '17:18', '17:21', '17:23', '17:25', '17:28', '17:31', '17:33', '17:36', '17:38', '17:40', '17:43', '17:51'],
    220: ['17:30', '17:35', '17:38', '17:40', '17:43', '17:45', '17:48', '17:51', '17:53', '17:55', '17:58', '18:01', '18:03', '18:06', '18:08', '18:10', '18:13', '18:21'],
    222: ['17:55', '18:00', '18:03', '18:04', '18:06', '18:08', '18:11', '', '', '', '', '18:16', '', '18:21', '', '', '', '18:36'],
    228: ['18:05', '18:10', '18:13', '18:15', '18:18', '18:20', '18:23', '18:26', '18:28', '18:30', '18:33', '18:36', '18:38', '18:41', '18:43', '18:45', '18:48', '18:56'],
    230: ['18:30', '18:35', '18:38', '18:40', '18:43', '18:45', '18:48', '18:51', '18:53', '18:55', '18:58', '19:01', '19:03', '19:06', '19:08', '19:10', '19:13', '19:21'],
    234: ['19:00', '19:05', '19:08', '19:10', '19:13', '19:15', '19:18', '19:21', '19:23', '19:25', '19:28', '19:31', '19:33', '19:36', '19:38', '19:40', '19:43', '19:51'],
    236: ['19:30', '19:35', '19:38', '19:40', '19:43', '19:45', '19:48', '19:51', '19:53', '19:55', '19:58', '20:01', '', '20:05', '20:08', '20:10', '', '20:20'],
    240: ['20:00', '20:05', '20:08', '20:10', '20:13', '20:15', '20:18', '20:21', '20:23', '20:25', '20:28', '20:31', '', '20:35', '20:38', '20:40', '', '20:50'],
    242: ['20:30', '20:35', '20:38', '20:40', '20:42', '20:44', '20:47', '20:49', '20:51', '20:53', '20:57', '21:00', '', '21:04', '21:06', '21:08', '', '21:18'],
    246: ['21:15', '21:20', '21:23', '21:25', '21:27', '21:29', '21:32', '21:34', '21:36', '21:38', '21:42', '21:45', '', '21:49', '21:51', '21:53', '', '22:03'],
    250: ['22:00', '22:05', '22:08', '22:10', '22:12', '22:14', '22:17', '22:19', '22:21', '22:23', '22:27', '22:30', '', '22:34', '22:36', '22:38', '22:41', '22:49'],
}

def time_to_minutes(time_str):
    """Convert time string to minutes from midnight"""
    if not time_str or time_str == '':
        return None
    try:
        parts = time_str.split(':')
        return int(parts[0]) * 60 + int(parts[1])
    except (ValueError, IndexError):
        return None

def minutes_to_time(minutes):
    """Convert minutes from midnight to time string"""
    if minutes is None:
        return None
    hours = int(minutes // 60)
    mins = int(minutes % 60)
    return f"{hours:02d}:{mins:02d}"

def add_delay_to_time(time_str, delay_minutes):
    """Add delay to a time string"""
    if not time_str or time_str == '':
        return None
    minutes = time_to_minutes(time_str)
    if minutes is None:
        return None
    return minutes_to_time(minutes + delay_minutes)

def generate_realistic_delays(hour, is_weekend=False, weather_factor=1.0):
    """Generate realistic delays based on time of day and conditions"""

    # Base delay probabilities and magnitudes
    if 6 <= hour <= 9 or 17 <= hour <= 20:  # Rush hours
        base_delay = np.random.exponential(3.0) * weather_factor
        if np.random.random() < 0.3:  # 30% chance of significant delay
            base_delay += np.random.exponential(5.0)
    elif 22 <= hour or hour <= 5:  # Night/early morning
        base_delay = np.random.exponential(1.5) * weather_factor
    else:  # Off-peak
        base_delay = np.random.exponential(2.0) * weather_factor

    # Weekend effect
    if is_weekend:
        base_delay = base_delay * 0.7

    # Sometimes trains are early (negative delay)
    if np.random.random() < 0.1:
        base_delay = -np.random.exponential(1.0)

    return round(base_delay, 1)

# Tunisian public holidays (simplified list for 2023-2024)
tunisian_holidays = {
    # 2023
    '2023-01-01', '2023-01-14', '2023-03-20', '2023-04-09', '2023-04-21', '2023-04-22',
    '2023-05-01', '2023-06-28', '2023-06-29', '2023-07-25', '2023-08-13', '2023-08-28',
    '2023-10-15', '2023-12-17',
    # 2024
    '2024-01-01', '2024-01-14', '2024-03-20', '2024-03-31', '2024-04-09', '2024-04-10',
    '2024-05-01', '2024-06-16', '2024-06-17', '2024-07-25', '2024-08-02', '2024-08-15',
    '2024-10-15', '2024-12-17'
}

def should_train_run(train_id, date):
    """Determine if a train should run on a given date based on its category"""
    date_str = date.strftime('%Y-%m-%d')
    is_sunday = date.weekday() == 6
    is_saturday = date.weekday() == 5
    is_holiday = date_str in tunisian_holidays
    is_weekday = date.weekday() < 5  # Monday-Friday

    train_category = train_day_categories.get(train_id)
    if train_category == 'D':  # All days
        return True
    elif train_category == 'A':  # All days except Sundays and holidays
        return not (is_sunday or is_holiday)
    elif train_category == 'B':  # Sundays and holidays only
        return is_sunday or is_holiday
    elif train_category == 'C':  # Weekdays only (no Saturdays, Sundays, or holidays)
        return is_weekday and not is_holiday

    else:
        return False

def get_day_category(train_id):
    return train_day_categories.get(train_id)

# Generate fake data
fake_data = []
start_date = datetime(2023, 1, 1)
end_date = datetime(2024, 12, 31)

# Generate data for each day
current_date = start_date
while current_date <= end_date:

    # Skip some days randomly (service disruptions)
    if np.random.random() < 0.02:  # 2% chance of no service
        current_date += timedelta(days=1)
        continue


    is_weekend = current_date.weekday() >= 5

    # Weather factor (simulate seasonal and random weather effects)
    season_factor = 1.0
    if current_date.month in [12, 1, 2]:  # Winter
        season_factor = 1.3
    elif current_date.month in [6, 7, 8]:  # Summer
        season_factor = 1.1

    # Random weather events
    weather_factor = season_factor * np.random.lognormal(0, 0.2)

    # Only include trains that should run on this day
    available_trains = [train_id for train_id in schedule_data.keys()
                       if (should_train_run(train_id, current_date) or train_day_categories[train_id]=='D')]

    # Sometimes cancel some trains randomly
    if available_trains:
        available_trains = random.sample(available_trains,
                                       random.randint(max(1, len(available_trains) - 3),
                                                     len(available_trains)))

    for train_id in available_trains:
        if train_id not in schedule_data:
            continue

        schedule = schedule_data[train_id]
        day_category = get_day_category(train_id)

        # Generate initial delay at first station
        first_station_time = schedule[0]
        if not first_station_time:
            continue

        first_station_hour = time_to_minutes(first_station_time)
        if first_station_hour is None:
            continue
        first_station_hour = first_station_hour // 60

        initial_delay = generate_realistic_delays(first_station_hour, is_weekend, weather_factor)

        # Track cumulative delay through the journey
        cumulative_delay = initial_delay

        for i, station in enumerate(stations):
            if i >= len(schedule) or not schedule[i] or schedule[i] == '':
                continue

            scheduled_time = schedule[i]

            # For first station, only departure
            if i == 0:
                actual_departure = add_delay_to_time(scheduled_time, cumulative_delay)

                fake_data.append({
                    'train_id': train_id,
                    'date': current_date.strftime('%Y-%m-%d'),
                    'day': day_category,
                    'station': station,
                    'scheduled_arrival': None,
                    'actual_arrival': None,
                    'scheduled_departure': scheduled_time,
                    'actual_departure': actual_departure,
                    'arrival_delay': None,
                    'departure_delay': round(cumulative_delay, 1)
                })

            # For last station, only arrival
            elif i == len(stations) - 1:
                actual_arrival = add_delay_to_time(scheduled_time, cumulative_delay)

                fake_data.append({
                    'train_id': train_id,
                    'date': current_date.strftime('%Y-%m-%d'),
                    'day': day_category,
                    'station': station,
                    'scheduled_arrival': scheduled_time,
                    'actual_arrival': actual_arrival,
                    'scheduled_departure': None,
                    'actual_departure': None,
                    'arrival_delay': round(cumulative_delay, 1),
                    'departure_delay': None
                })

            # For intermediate stations, both arrival and departure
            else:
                # Arrival delay accumulates
                actual_arrival = add_delay_to_time(scheduled_time, cumulative_delay)

                # Departure might have additional delay (dwell time issues)
                additional_delay = 0
                if np.random.random() < 0.15:  # 15% chance of dwell time delay
                    additional_delay = np.random.exponential(1.0)

                # Sometimes trains make up time between stations
                if np.random.random() < 0.1:  # 10% chance of recovery
                    recovery = np.random.exponential(1.0)
                    cumulative_delay = max(0, cumulative_delay - recovery)

                departure_delay = cumulative_delay + additional_delay
                actual_departure = add_delay_to_time(scheduled_time, departure_delay)

                fake_data.append({
                    'train_id': train_id,
                    'date': current_date.strftime('%Y-%m-%d'),
                    'day': day_category,
                    'station': station,
                    'scheduled_arrival': scheduled_time,
                    'actual_arrival': actual_arrival,
                    'scheduled_departure': scheduled_time,  # Same as arrival for intermediate stations
                    'actual_departure': actual_departure,
                    'arrival_delay': round(cumulative_delay, 1),
                    'departure_delay': round(departure_delay, 1)
                })

                cumulative_delay = departure_delay

                # Add some random variation for next segment
                cumulative_delay += np.random.normal(0, 0.5)




    current_date += timedelta(days=1)

# Create DataFrame
df = pd.DataFrame(fake_data)

# Display first few rows
print("Generated TGM Train Delay Data Sample:")
print("=" * 80)
print(df.head(20).to_string(index=False))
print(f"\nTotal records: {len(df)}")
print(f"Date range: {df['date'].min()} to {df['date'].max()}")
print(f"Unique trains: {df['train_id'].nunique()}")
print(f"Unique stations: {df['station'].nunique()}")

# Save to CSV
df.to_csv('tgm_train_delays.csv', index=False)
print("\nData saved to 'tgm_train_delays.csv'")

# Show some statistics
print("\nDelay Statistics:")
print("=" * 30)
departure_delays = df['departure_delay'].dropna()
arrival_delays = df['arrival_delay'].dropna()

print(f"Average departure delay: {departure_delays.mean():.1f} minutes")
print(f"Average arrival delay: {arrival_delays.mean():.1f} minutes")
print(f"Max departure delay: {departure_delays.max():.1f} minutes")
print(f"Max arrival delay: {arrival_delays.max():.1f} minutes")
print(f"Percentage of on-time departures (â‰¤1 min): {(departure_delays <= 1).mean()*100:.1f}%")
print(f"Percentage of on-time arrivals (â‰¤1 min): {(arrival_delays <= 1).mean()*100:.1f}%")

df.head()

df['day'].value_counts()

df['scheduled_arrival'] = df['scheduled_arrival'].fillna(df['scheduled_departure'])

# Replace None in actual_arrival with actual_departure
df['actual_arrival'] = df['actual_arrival'].fillna(df['actual_departure'])

# Optional: if you also want to fill missing departures with arrival times
df['scheduled_departure'] = df['scheduled_departure'].fillna(df['scheduled_arrival'])
df['actual_departure'] = df['actual_departure'].fillna(df['actual_arrival'])
df

df['departure_delay'] = df['departure_delay'].fillna(df['arrival_delay'])

# Replace None in actual_arrival with actual_departure
df['arrival_delay'] = df['arrival_delay'].fillna(df['departure_delay'])

df.info()

df.to_csv('tgm_train_delays.csv', index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')


print("ðŸš„ COMPREHENSIVE TRAIN DELAY PREDICTION PIPELINE")
print("=" * 60)

# ============================================================================
# 1. EXPLORATORY DATA ANALYSIS (EDA)
# ============================================================================

print("\n1. INITIAL DATA EXPLORATION")
print("-" * 30)
print(f"Dataset shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")

print("\nDataset Info:")
print(df.info())

print("\nFirst few rows:")
print(df.head())

print("\nSummary Statistics:")
print(df.describe())

print("\nMissing Values:")
print(df.isnull().sum())

print("\nUnique Values per Column:")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# ============================================================================
# 2. DATA PREPROCESSING & CLEANING
# ============================================================================

print("\n2. DATA PREPROCESSING")
print("-" * 30)

# Remove unnecessary columns as specified
unnecessary_cols = ['actual_arrival', 'actual_departure', 'scheduled_departure']
df_clean = df.drop(columns=unnecessary_cols)

print(f"Removed columns: {unnecessary_cols}")
print(f"New dataset shape: {df_clean.shape}")

# Convert date to datetime
df_clean['date'] = pd.to_datetime(df_clean['date'])

# Extract time features from scheduled_arrival
df_clean['scheduled_hour'] = pd.to_datetime(df_clean['scheduled_arrival'], format='%H:%M').dt.hour
df_clean['scheduled_minute'] = pd.to_datetime(df_clean['scheduled_arrival'], format='%H:%M').dt.minute

# Create time-based features
df_clean['time_of_day'] = pd.cut(df_clean['scheduled_hour'],
                                bins=[0, 6, 12, 18, 24],
                                labels=['Night', 'Morning', 'Afternoon', 'Evening'])

# Extract date features
df_clean['month'] = df_clean['date'].dt.month
df_clean['day_of_week'] = df_clean['date'].dt.dayofweek
df_clean['is_weekend'] = (df_clean['day_of_week'] >= 5).astype(int)

print("\nNew features created:")
print("- scheduled_hour, scheduled_minute")
print("- time_of_day (categorical)")
print("- month, day_of_week, is_weekend")

# ============================================================================
# 3. FEATURE ENGINEERING
# ============================================================================

print("\n3. ADVANCED FEATURE ENGINEERING")
print("-" * 30)

# Create station sequence features
df_clean['station_order'] = df_clean.groupby('train_id').cumcount()
df_clean['total_stations'] = df_clean.groupby('train_id')['station'].transform('count')
df_clean['station_progress'] = df_clean['station_order'] / df_clean['total_stations']

# Previous delay features (lag features)
df_clean = df_clean.sort_values(['train_id', 'station_order'])
df_clean['prev_arrival_delay'] = df_clean.groupby('train_id')['arrival_delay'].shift(1)
df_clean['prev_departure_delay'] = df_clean.groupby('train_id')['departure_delay'].shift(1)

# Rolling averages of delays
df_clean['rolling_avg_arrival_delay'] = df_clean.groupby('train_id')['arrival_delay'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)
df_clean['rolling_avg_departure_delay'] = df_clean.groupby('train_id')['departure_delay'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)

# Station-based features
station_stats = df_clean.groupby('station').agg({
    'arrival_delay': ['mean', 'std'],
    'departure_delay': ['mean', 'std']
}).round(2)

station_stats.columns = ['station_avg_arrival_delay', 'station_std_arrival_delay',
                        'station_avg_departure_delay', 'station_std_departure_delay']

df_clean = df_clean.merge(station_stats, on='station', how='left')

# Train-based features
train_stats = df_clean.groupby('train_id').agg({
    'arrival_delay': ['mean', 'std'],
    'departure_delay': ['mean', 'std']
}).round(2)

train_stats.columns = ['train_avg_arrival_delay', 'train_std_arrival_delay',
                      'train_avg_departure_delay', 'train_std_departure_delay']

df_clean = df_clean.merge(train_stats, on='train_id', how='left')

# Rush hour feature
df_clean['is_rush_hour'] = ((df_clean['scheduled_hour'].between(7, 9)) |
                           (df_clean['scheduled_hour'].between(17, 19))).astype(int)

# Fill NaN values for lag features
df_clean['prev_arrival_delay'].fillna(0, inplace=True)
df_clean['prev_departure_delay'].fillna(0, inplace=True)

print("Advanced features created:")
print("- station_order, total_stations, station_progress")
print("- prev_arrival_delay, prev_departure_delay (lag features)")
print("- rolling_avg_arrival_delay, rolling_avg_departure_delay")
print("- station_avg/std_arrival/departure_delay")
print("- train_avg/std_arrival/departure_delay")
print("- is_rush_hour")

# ============================================================================
# 4. VISUALIZATION & INSIGHTS
# ============================================================================

print("\n4. DATA VISUALIZATION & INSIGHTS")
print("-" * 30)

# Set up the plotting style
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Delay distribution
axes[0, 0].hist(df_clean['arrival_delay'], bins=30, alpha=0.7, color='skyblue', label='Arrival Delay')
axes[0, 0].hist(df_clean['departure_delay'], bins=30, alpha=0.7, color='lightcoral', label='Departure Delay')
axes[0, 0].set_title('Distribution of Delays')
axes[0, 0].set_xlabel('Delay (minutes)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].legend()

# 2. Delay by time of day
delay_by_time = df_clean.groupby('time_of_day')[['arrival_delay', 'departure_delay']].mean()
delay_by_time.plot(kind='bar', ax=axes[0, 1])
axes[0, 1].set_title('Average Delay by Time of Day')
axes[0, 1].set_ylabel('Average Delay (minutes)')
axes[0, 1].tick_params(axis='x', rotation=45)

# 3. Delay by station
top_stations = df_clean.groupby('station')['arrival_delay'].mean().sort_values(ascending=False).head(10)
top_stations.plot(kind='bar', ax=axes[0, 2])
axes[0, 2].set_title('Top 10 Stations by Average Arrival Delay')
axes[0, 2].set_ylabel('Average Delay (minutes)')
axes[0, 2].tick_params(axis='x', rotation=45)

# 4. Correlation heatmap
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
corr_matrix = df_clean[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, ax=axes[1, 0])
axes[1, 0].set_title('Correlation Matrix')

# 5. Delay progression through journey
journey_delay = df_clean.groupby('station_progress')[['arrival_delay', 'departure_delay']].mean()
journey_delay.plot(ax=axes[1, 1])
axes[1, 1].set_title('Delay Progression Through Journey')
axes[1, 1].set_xlabel('Station Progress (0=Start, 1=End)')
axes[1, 1].set_ylabel('Average Delay (minutes)')

# 6. Box plot of delays by day type
df_clean['day_type'] = df_clean['day'].map({'B': 'Weekday', 'D': 'Weekend', 'A': np.nan, 'C': np.nan}) # Added mapping for 'A' and 'C' to handle potential NaNs from those day types
sns.boxplot(data=df_clean, x='day_type', y='arrival_delay', ax=axes[1, 2])
axes[1, 2].set_title('Arrival Delay by Day Type')

plt.tight_layout()
plt.show()

# ============================================================================
# 5. PREPARE DATA FOR MODELING
# ============================================================================

print("\n5. MODEL PREPARATION")
print("-" * 30)

# Define features and target
# Remove non-predictive columns and 'day_type'
exclude_cols = ['date', 'scheduled_arrival', 'arrival_delay', 'departure_delay', 'day_type']
feature_cols = [col for col in df_clean.columns if col not in exclude_cols]

# Choose target variable (let's predict arrival_delay)
target = 'arrival_delay'
X = df_clean[feature_cols].copy()
y = df_clean[target].copy()

print(f"Target variable: {target}")
print(f"Number of features: {len(feature_cols)}")
print(f"Features: {feature_cols}")

# Handle categorical variables
categorical_features = ['day', 'station', 'time_of_day']
numerical_features = [col for col in feature_cols if col not in categorical_features]

print(f"\nCategorical features: {categorical_features}")
print(f"Numerical features: {numerical_features}")

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ]
)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTraining set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# ============================================================================
# 6. MODEL TRAINING & EVALUATION
# ============================================================================

print("\n6. MODEL TRAINING & EVALUATION")
print("-" * 30)

# Define models to test
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate models
results = {}
best_model = None
best_score = float('-inf')

for name, model in models.items():
    # Create pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions
    y_pred = pipeline.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Cross-validation score
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')

    results[name] = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'CV_R2_mean': cv_scores.mean(),
        'CV_R2_std': cv_scores.std()
    }

    # Track best model
    if r2 > best_score:
        best_score = r2
        best_model = (name, pipeline)

    print(f"\n{name} Results:")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  CV RÂ² (meanÂ±std): {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# ============================================================================
# 7. HYPERPARAMETER TUNING FOR BEST MODEL
# ============================================================================

print(f"\n7. HYPERPARAMETER TUNING - {best_model[0]}")
print("-" * 30)

if best_model[0] == 'Random Forest':
    # Random Forest hyperparameter tuning
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [10, 20, None],
        'regressor__min_samples_split': [2, 5, 10]
    }
elif best_model[0] == 'Gradient Boosting':
    # Gradient Boosting hyperparameter tuning
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__learning_rate': [0.01, 0.1, 0.2],
        'regressor__max_depth': [3, 5, 7]
    }
else:
    # Ridge/Lasso hyperparameter tuning
    param_grid = {
        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]
    }

# Perform grid search
grid_search = GridSearchCV(best_model[1], param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model evaluation
best_tuned_model = grid_search.best_estimator_
y_pred_tuned = best_tuned_model.predict(X_test)

final_r2 = r2_score(y_test, y_pred_tuned)
final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))
final_mae = mean_absolute_error(y_test, y_pred_tuned)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation RÂ² score: {grid_search.best_score_:.4f}")
print(f"\nFinal Model Performance:")
print(f"  RÂ² Score: {final_r2:.4f}")
print(f" rmse: {final_rmse:.4f}")
print(f"  MAE: {final_mae:.4f}")