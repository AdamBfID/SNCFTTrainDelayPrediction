# -*- coding: utf-8 -*-
"""HACKATHON_LESENSITIENS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NlK-1XjCAVMbxUhPL6RtjVbAw-AfBKzb
"""

import pandas as pd
df = pd.read_csv('final.csv')
df.head()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')


print("ðŸš„ COMPREHENSIVE TRAIN DELAY PREDICTION PIPELINE")
print("=" * 60)

# ============================================================================
# 1. EXPLORATORY DATA ANALYSIS (EDA)
# ============================================================================

print("\n1. INITIAL DATA EXPLORATION")
print("-" * 30)
print(f"Dataset shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")

print("\nDataset Info:")
print(df.info())

print("\nFirst few rows:")
print(df.head())

print("\nSummary Statistics:")
print(df.describe())

print("\nMissing Values:")
print(df.isnull().sum())

print("\nUnique Values per Column:")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# ============================================================================
# 2. DATA PREPROCESSING & CLEANING
# ============================================================================

print("\n2. DATA PREPROCESSING")
print("-" * 30)

# Remove unnecessary columns as specified
unnecessary_cols = ['actual_arrival', 'actual_departure', 'scheduled_departure']
df_clean = df.drop(columns=unnecessary_cols)

print(f"Removed columns: {unnecessary_cols}")
print(f"New dataset shape: {df_clean.shape}")

# Convert date to datetime
df_clean['date'] = pd.to_datetime(df_clean['date'])

# Extract time features from scheduled_arrival
df_clean['scheduled_hour'] = pd.to_datetime(df_clean['scheduled_arrival'], format='%H:%M').dt.hour
df_clean['scheduled_minute'] = pd.to_datetime(df_clean['scheduled_arrival'], format='%H:%M').dt.minute

# Create time-based features
df_clean['time_of_day'] = pd.cut(df_clean['scheduled_hour'],
                                bins=[0, 6, 12, 18, 24],
                                labels=['Night', 'Morning', 'Afternoon', 'Evening'])

# Extract date features
df_clean['month'] = df_clean['date'].dt.month
df_clean['day_of_week'] = df_clean['date'].dt.dayofweek
df_clean['is_weekend'] = (df_clean['day_of_week'] >= 5).astype(int)

print("\nNew features created:")
print("- scheduled_hour, scheduled_minute")
print("- time_of_day (categorical)")
print("- month, day_of_week, is_weekend")

# ============================================================================
# 3. FEATURE ENGINEERING
# ============================================================================

print("\n3. ADVANCED FEATURE ENGINEERING")
print("-" * 30)

# Create station sequence features
df_clean['station_order'] = df_clean.groupby('train_id').cumcount()
df_clean['total_stations'] = df_clean.groupby('train_id')['station'].transform('count')
df_clean['station_progress'] = df_clean['station_order'] / df_clean['total_stations']

# Previous delay features (lag features)
df_clean = df_clean.sort_values(['train_id', 'station_order'])
df_clean['prev_arrival_delay'] = df_clean.groupby('train_id')['arrival_delay'].shift(1)
df_clean['prev_departure_delay'] = df_clean.groupby('train_id')['departure_delay'].shift(1)

# Rolling averages of delays
df_clean['rolling_avg_arrival_delay'] = df_clean.groupby('train_id')['arrival_delay'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)
df_clean['rolling_avg_departure_delay'] = df_clean.groupby('train_id')['departure_delay'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)

# Station-based features
station_stats = df_clean.groupby('station').agg({
    'arrival_delay': ['mean', 'std'],
    'departure_delay': ['mean', 'std']
}).round(2)

station_stats.columns = ['station_avg_arrival_delay', 'station_std_arrival_delay',
                        'station_avg_departure_delay', 'station_std_departure_delay']

df_clean = df_clean.merge(station_stats, on='station', how='left')

# Train-based features
train_stats = df_clean.groupby('train_id').agg({
    'arrival_delay': ['mean', 'std'],
    'departure_delay': ['mean', 'std']
}).round(2)

train_stats.columns = ['train_avg_arrival_delay', 'train_std_arrival_delay',
                      'train_avg_departure_delay', 'train_std_departure_delay']

df_clean = df_clean.merge(train_stats, on='train_id', how='left')

# Rush hour feature
df_clean['is_rush_hour'] = ((df_clean['scheduled_hour'].between(7, 9)) |
                           (df_clean['scheduled_hour'].between(17, 19))).astype(int)

# Fill NaN values for lag features
df_clean['prev_arrival_delay'].fillna(0, inplace=True)
df_clean['prev_departure_delay'].fillna(0, inplace=True)

print("Advanced features created:")
print("- station_order, total_stations, station_progress")
print("- prev_arrival_delay, prev_departure_delay (lag features)")
print("- rolling_avg_arrival_delay, rolling_avg_departure_delay")
print("- station_avg/std_arrival/departure_delay")
print("- train_avg/std_arrival/departure_delay")
print("- is_rush_hour")

# ============================================================================
# 4. VISUALIZATION & INSIGHTS
# ============================================================================

print("\n4. DATA VISUALIZATION & INSIGHTS")
print("-" * 30)

# Set up the plotting style
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Delay distribution
axes[0, 0].hist(df_clean['arrival_delay'], bins=30, alpha=0.7, color='skyblue', label='Arrival Delay')
axes[0, 0].hist(df_clean['departure_delay'], bins=30, alpha=0.7, color='lightcoral', label='Departure Delay')
axes[0, 0].set_title('Distribution of Delays')
axes[0, 0].set_xlabel('Delay (minutes)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].legend()

# 2. Delay by time of day
delay_by_time = df_clean.groupby('time_of_day')[['arrival_delay', 'departure_delay']].mean()
delay_by_time.plot(kind='bar', ax=axes[0, 1])
axes[0, 1].set_title('Average Delay by Time of Day')
axes[0, 1].set_ylabel('Average Delay (minutes)')
axes[0, 1].tick_params(axis='x', rotation=45)

# 3. Delay by station
top_stations = df_clean.groupby('station')['arrival_delay'].mean().sort_values(ascending=False).head(10)
top_stations.plot(kind='bar', ax=axes[0, 2])
axes[0, 2].set_title('Top 10 Stations by Average Arrival Delay')
axes[0, 2].set_ylabel('Average Delay (minutes)')
axes[0, 2].tick_params(axis='x', rotation=45)

# 4. Correlation heatmap
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
corr_matrix = df_clean[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, ax=axes[1, 0])
axes[1, 0].set_title('Correlation Matrix')

# 5. Delay progression through journey
journey_delay = df_clean.groupby('station_progress')[['arrival_delay', 'departure_delay']].mean()
journey_delay.plot(ax=axes[1, 1])
axes[1, 1].set_title('Delay Progression Through Journey')
axes[1, 1].set_xlabel('Station Progress (0=Start, 1=End)')
axes[1, 1].set_ylabel('Average Delay (minutes)')

# 6. Box plot of delays by day type
df_clean['day_type'] = df_clean['day'].map({'B': 'Weekday', 'D': 'Weekend', 'A': np.nan, 'C': np.nan}) # Added mapping for 'A' and 'C' to handle potential NaNs from those day types
sns.boxplot(data=df_clean, x='day_type', y='arrival_delay', ax=axes[1, 2])
axes[1, 2].set_title('Arrival Delay by Day Type')

plt.tight_layout()
plt.show()

# ============================================================================
# 5. PREPARE DATA FOR MODELING
# ============================================================================

print("\n5. MODEL PREPARATION")
print("-" * 30)

# Define features and target
# Remove non-predictive columns and 'day_type'
exclude_cols = ['date', 'scheduled_arrival', 'arrival_delay', 'departure_delay', 'day_type']
feature_cols = [col for col in df_clean.columns if col not in exclude_cols]

# Choose target variable (let's predict arrival_delay)
target = 'arrival_delay'
X = df_clean[feature_cols].copy()
y = df_clean[target].copy()

print(f"Target variable: {target}")
print(f"Number of features: {len(feature_cols)}")
print(f"Features: {feature_cols}")

# Handle categorical variables
categorical_features = ['day', 'station', 'time_of_day']
numerical_features = [col for col in feature_cols if col not in categorical_features]

print(f"\nCategorical features: {categorical_features}")
print(f"Numerical features: {numerical_features}")

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ]
)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTraining set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# ============================================================================
# 6. MODEL TRAINING & EVALUATION
# ============================================================================

print("\n6. MODEL TRAINING & EVALUATION")
print("-" * 30)

# Define models to test
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate models
results = {}
best_model = None
best_score = float('-inf')

for name, model in models.items():
    # Create pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions
    y_pred = pipeline.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Cross-validation score
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')

    results[name] = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'CV_R2_mean': cv_scores.mean(),
        'CV_R2_std': cv_scores.std()
    }

    # Track best model
    if r2 > best_score:
        best_score = r2
        best_model = (name, pipeline)

    print(f"\n{name} Results:")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  CV RÂ² (meanÂ±std): {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# ============================================================================
# 7. HYPERPARAMETER TUNING FOR BEST MODEL
# ============================================================================

print(f"\n7. HYPERPARAMETER TUNING - {best_model[0]}")
print("-" * 30)

if best_model[0] == 'Random Forest':
    # Random Forest hyperparameter tuning
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [10, 20, None],
        'regressor__min_samples_split': [2, 5, 10]
    }
elif best_model[0] == 'Gradient Boosting':
    # Gradient Boosting hyperparameter tuning
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__learning_rate': [0.01, 0.1, 0.2],
        'regressor__max_depth': [3, 5, 7]
    }
else:
    # Ridge/Lasso hyperparameter tuning
    param_grid = {
        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]
    }

# Perform grid search
grid_search = GridSearchCV(best_model[1], param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model evaluation
best_tuned_model = grid_search.best_estimator_
y_pred_tuned = best_tuned_model.predict(X_test)

final_r2 = r2_score(y_test, y_pred_tuned)
final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))
final_mae = mean_absolute_error(y_test, y_pred_tuned)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation RÂ² score: {grid_search.best_score_:.4f}")
print(f"\nFinal Model Performance:")
print(f"  RÂ² Score: {final_r2:.4f}")
print(f" rmse: {final_rmse:.4f}")
print(f"  MAE: {final_mae:.4f}")

